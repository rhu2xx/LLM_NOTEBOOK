{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13dba67",
   "metadata": {},
   "source": [
    "# Pytorch internals\n",
    "`Tensor` is the central data structure in PyTorch. The internals of `torch.tensor` involves\n",
    "1. data\n",
    "2. metadata\n",
    "   1. the size of tensor(`a.shape`)\n",
    "   2. the type of elements(`a.dtype`)\n",
    "   3. the device of tensor(`a.device`)\n",
    "   4. the layout of tensor(`a.layout`)\n",
    "   5. the stride of tensor(`a.stride`)\n",
    "\n",
    "## Memory structure\n",
    "The `torch.tensor` class is backed by a low-level implementation in C++. So we can regard this as a C++ object with multiple member variables and functions. \n",
    "\n",
    "> `torch.tensor` is just an another more elegant wrapper of `pointer`. \n",
    "\n",
    "The default layout is row-major, and the physical memory is always one-dimensional. So like `pointer` in C++, the position of the element `a[i,j,k]` in `torch.tensor` is indexed by $i \\times stride[0] + j\\times stride[1] + k\\times stride[2]$.\n",
    "\n",
    "## Tensor low-level Operations\n",
    "At the most abstract level, when you call `torch.mm`, two dispatches happen:\n",
    "1. The first dispatch is based on the device type and layout of tensors. The device type determines where the computation will be performed (e.g., CPU or GPU), while the layout defines how the tensor data is organized in memory (e.g., strided tensors or sparse tensors). It is a dynamic dispatch mechanism that allows PyTorch to choose the appropriate implementation for the given input tensors.\n",
    "2. The second dispatch is based on the data type of the tensors. This includes information about the precision of the data (e.g., float32, float64, int32, etc.) and any other relevant properties. This dispatch ensures that the correct implementation is chosen for the specific data types involved in the operation.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/rhu2xx/picx-images-hosting/raw/master/20251014/tensor_dispatch.pfqamzmoj.png\" alt=\"tensor_dispatch\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Tensor extensions\n",
    "Except dense tensor, PyTorch also supports various other tensor types, including XLA tensors, quantized tensor, sparse tensors, and MKL-DNN tensors.\n",
    "\n",
    "The trinity three parameters which uniquely determine what a tensor is.\n",
    "| Name | Definition | Example |\n",
    "|------|------|------|\n",
    "| **device** | where the tensor is stored | `'cpu'`, `'cuda:0'`, `'xla'`, `'mps'` |\n",
    "| **layout** | how the tensor is laid out in memory | `torch.strided`, `torch.sparse_coo`, `torch.sparse_csr`, `torch._mkldnn` |\n",
    "| **dtype** | the data type of each element | `torch.float32`, `torch.int8`, `torch.qint8`, etc. |\n",
    "\n",
    "> The cartesian product of these three parameters defines the full space of tensor types in PyTorch.\n",
    "> **Tensor = function(device, layout, dtype)**\n",
    "\n",
    "Except extensions (create a new tensor class), we can also write a wrapper class around Pytorch tensors that implements our object type.\n",
    "\n",
    "## Differences between tensor wrapper and extending pytorch\n",
    "| Feature | **Tensor Wrapper** | **Extending PyTorch** |\n",
    "|----------|--------------------|------------------------|\n",
    "| **What it is** | A normal Python class that holds a Tensor inside | A new Tensor type added inside PyTorch‚Äôs core |\n",
    "| **How it works** | Uses an existing Tensor (just wraps it) | Changes how Tensor is built or stored (C++ level) |\n",
    "| **Autograd (gradient)** | ‚ùå Wrapper itself doesn‚Äôt get gradients | ‚úÖ Fully supports autograd and gradients |\n",
    "| **Use case** | Add simple features like logging, printing, unit labels | Add new behavior like new device, layout, or data type |\n",
    "| **Difficulty** | üü¢ Easy (Python only) | üî¥ Hard (needs C++ knowledge) |\n",
    "| **Example** | `class MyTensor:` that wraps a Tensor | `torch.sparse_coo`, quantized Tensor |\n",
    "| **Speed** | Same as normal Tensor | May be faster or slower depending on backend |\n",
    "| **When to use** | When you only need extra Python logic | When you need new Tensor behavior or backend support |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f711a4",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "A gradient is a slope -- it tells your how much something changes when you change its input. The gradients is the rate of change of a function with respect to its inputs.\n",
    "\n",
    "In deep learning, we want our model to learn good weights. To do that, we ask:\n",
    "> If I change this weight a little, will my loss get bigger or smaller?\n",
    "\n",
    "So the update rule in training is:\n",
    "$$\n",
    "new\\_weight = old\\_weight - learning\\_rate \\times gradient\n",
    "$$\n",
    "\n",
    "\n",
    "### Forward Pass\n",
    "1. put data into the model\n",
    "2. flows through all the layers\n",
    "3. get an output and compute a loss\n",
    "\n",
    "```python\n",
    "y_pred = model(x)\n",
    "loss = criterion(y_pred, y_true)\n",
    "```\n",
    "### Backward Pass\n",
    "1. compute the gradient of the loss with respect to the model parameters.\n",
    "2. Apply the chain rule to compute how each weight affected the final loss\n",
    "3. Store those derivatives(gradient) in each parameter's `.grad`\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "4. update the model parameters using the gradients\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "## Autograd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e2b71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\llm\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:275: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó„ÄÇ (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2      # y = x¬≤\n",
    "y.backward()   # compute dy/dx\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fb5ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass:\n",
      "x = 2.0, y = 7.0, z = 343.0\n",
      "\n",
      "Backward pass (gradients):\n",
      "x.grad = 294.0\n",
      "y.grad = 147.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1Ô∏è‚É£ Make input tensor that tracks gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Forward pass (PyTorch builds the graph automatically)\n",
    "y = 2 * x + 3     # Step 1\n",
    "y.retain_grad()\n",
    "z = y ** 3        # Step 2\n",
    "print(\"Forward pass:\")\n",
    "print(f\"x = {x.item()}, y = {y.item()}, z = {z.item()}\")\n",
    "\n",
    "# 3Ô∏è‚É£ Backward pass\n",
    "z.backward()       # Compute dz/dx\n",
    "print(\"\\nBackward pass (gradients):\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"y.grad = {y.grad}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}