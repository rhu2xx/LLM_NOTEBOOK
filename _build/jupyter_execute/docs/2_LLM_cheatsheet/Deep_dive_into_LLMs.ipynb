{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb586e7e",
   "metadata": {},
   "source": [
    "# Depp dive into LLMs\n",
    "This is the note for the video [Deep dive into LLMs: How they work, how to build one, and what's next](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=1s) by Yannic Kilcher.\n",
    "\n",
    "## Pretraining stage \n",
    "\n",
    "### step 1: download and preprocess the internet\n",
    "- `Finneweb` is a large-scale web corpus built by Hugging face (44TB disk space). Common Crawl is the main data source. \n",
    "\n",
    "\n",
    "### step 2: tokenization\n",
    "- The byte pair encoding (BPE) is used to tokenize the text data. We need expanded symbol diversity and shorter token sequences. We pair the most frequent bytes together to form new tokens iteratively. So the size of a vacabulary is extended from 256 to 50,000. \n",
    "![](https://github.com/rhu2xx/picx-images-hosting/raw/master/image.2vf67bk8fb.webp)\n",
    "\n",
    "### step 3: neural network training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}